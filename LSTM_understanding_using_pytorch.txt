class LSTMScratch(d2l.Module):
    def __init__(self, num_inputs, num_hiddens, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()

explanation:

class LSTMScratch(d2l.Module):

lass Definition: This defines a new class LSTMScratch that inherits from d2l.Module, which could be a module from the d2l framework that extends PyTorch's functionality.

এটি একটি নতুন ক্লাস সংজ্ঞায়িত করে LSTMScratchযা থেকে উত্তরাধিকারসূত্রে পাওয়া যায় d2l.Module, যা ফ্রেমওয়ার্ক থেকে একটি মডিউল হতে পারে d2lযা PyTorch এর কার্যকারিতা প্রসারিত করে।

def __init__(self, num_inputs, num_hiddens, sigma=0.01):

Initializes the LSTM module. It takes three arguments:
num_inputs: The number of input features (dimensions) for the input data.
num_hiddens: The number of hidden units in the LSTM cell, which determines the size of the hidden state.
sigma: A scaling factor used when initializing the weights, defaulting to 0.01.

super().__init__():

This calls the constructor of the parent class (d2l.Module), which might handle some internal setup.

self.save_hyperparameters():

Saves the hyperparameters num_inputs, num_hiddens, and sigma for later use, possibly for reproducibility or analysis.


init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)
triple = lambda: (init_weight(num_inputs, num_hiddens),
                  init_weight(num_hiddens, num_hiddens),
                  nn.Parameter(torch.zeros(num_hiddens)))

explain:

init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)

init_weight: A lambda function used to initialize a weight matrix with a normal distribution, scaled by sigma. It uses torch.randn to generate random values from a standard normal distribution, and nn.Parameter makes the weights learnable by registering them as PyTorch parameters.
The *shape argument allows for flexible shape definition (e.g., number of input features and hidden units).
triple: Another lambda function that returns a tuple of three parameters (weights and bias) for each gate:
The first matrix W_x (input-to-hidden weight matrix) of shape (num_inputs, num_hiddens).
The second matrix W_h (hidden-to-hidden weight matrix) of shape (num_hiddens, num_hiddens).
A bias vector b initialized to zeros of size (num_hiddens).
3. Gates in LSTM
python
Copy code
self.W_xi, self.W_hi, self.b_i = triple() # Input gate
self.W_xf, self.W_hf, self.b_f = triple() # Forget gate
self.W_xo, self.W_ho, self.b_o = triple() # Output gate
self.W_xc, self.W_hc, self.b_c = triple() # Input node
LSTMs have four key components (gates and input node), and each one has corresponding weight matrices and bias terms:

Input Gate (i): Controls how much of the input should be used to update the cell state.
W_xi: Weight matrix for input-to-hidden.
W_hi: Weight matrix for hidden-to-hidden.
b_i: Bias for the input gate.
Forget Gate (f): Determines how much of the previous cell state to forget.
W_xf: Weight matrix for input-to-hidden.
W_hf: Weight matrix for hidden-to-hidden.
b_f: Bias for the forget gate.
Output Gate (o): Controls how much of the cell state to output as the hidden state.
W_xo: Weight matrix for input-to-hidden.
W_ho: Weight matrix for hidden-to-hidden.
b_o: Bias for the output gate.
Input Node (c): The candidate cell state, which is used in conjunction with the input gate to update the cell state.
W_xc: Weight matrix for input-to-hidden.
W_hc: Weight matrix for hidden-to-hidden.
b_c: Bias for the input node.
Summary
The code defines a custom LSTM model where:

The class LSTMScratch initializes weight matrices and bias terms for the input gate, forget gate, output gate, and input node of the LSTM cell.
Each gate uses two weight matrices: one for the input (W_x) and one for the hidden state (W_h), as well as a bias (b).
The weights are initialized using a random normal distribution scaled by sigma, and the biases are initialized to zeros.
This setup forms the basic building blocks of an LSTM cell, allowing it to learn from sequential data by managing how information flows through its memory and gates.


This is my code for keras but i don'nt want to run keras anymore .i want to run pytorh .so make the code for pytorch but remaind it i want same result as keras code give me 

# Code Source: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/
# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    # forecast sequence (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
    # put it all together
    agg = concat(cols, axis=1)
    agg.columns = names
    # drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg

# create the reshape function
def reshape_data(train,test):
    #Frame as supervised learning and drop all time t columns except
    reframed_train = series_to_supervised(train, 1, 1)
    reframed_test = series_to_supervised(test, 1, 1)
    # split into train and test sets
    train= reframed_train.values
    test=reframed_test.values
    # split into input and outputs
    train_X, y_train = train[:, :-1], train[:, -1]
    test_X, y_test = test[:, :-1], test[:, -1]
    # reshape input to be 3D [samples, timesteps, features]
    x_train = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
    x_test = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
    return x_train,x_test,y_train,y_test

encoder = LabelEncoder()
#combine X train and Y train as train data 
train_data=pd.DataFrame()
train_data[X_train.columns]=X_train
train_data[Y_train.columns]=Y_train

#combine X test and Y test as test data 
test_data=pd.DataFrame()
test_data[X_test.columns]=X_test
test_data[Y_test.columns]=Y_test

# using the function to obtian reshaped x_train,x_test,y_train,y_test
x_train,x_test,y_train,y_test=reshape_data(train_data,test_data)

# design network for confirmed cases data 
model = Sequential()
model.add(LSTM(60, activation='relu',input_shape=(x_train.shape[1], x_train.shape[2])))

model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')
# fit network
history = model.fit(x_train, y_train, epochs=30, batch_size=50,  verbose=1, shuffle=False)


as a hints my fr given below 


